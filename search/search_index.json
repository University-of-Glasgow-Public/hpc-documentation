{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"University of Glasgow HPC","text":"<p>Welcome to the University of Glasgow HPC documentation. </p> <p>This website contains helpful user information and learning material about HPC in general and information specific to University fo Glasgow systems.</p> <p>You can use the Next and Previous buttons at the bottom of the page to go through the content in a structured way, or use the left-hand navigation to browse. At the top-right of the page is a search bar, which should help you find the content you are looking for. When on a page with subheadings, you will see a table of contents appear on the right hand side, which can help you navigate larger pages.</p> <p>In the grid below you can find some quick links to content users are frequently looking for:</p> <ul> <li> <p> Get Access to HPC</p> <p>Start accelerating your compute work!</p> <p> Getting an Account</p> </li> <li> <p> Start your Work</p> <p>This guide will help both novice and experienced users!</p> <p> Quickstart</p> </li> <li> <p> Need help?</p> <p>Open a HPC Support request through Ivanti, and get in contact with the admins!</p> <p> Support</p> </li> <li> <p> Stay Informed</p> <p>Join our Team on Microsoft Teams to be up to date on any news and find like-minded HPC users!</p> <p> Lochan Community</p> </li> </ul> <p>This page is managed by Research Computing as a Service, a function within the University of Glasgow, supporting researchers across all disciplines with their computing needs. More information on RCaaS can be found here.</p>"},{"location":"account-registration/","title":"Getting an Account","text":"Lochan <p>Any staff, student and affiliate user within the University of Glasgow is eligible to use Lochan. As affiliate / honorary you\u2019ll need a University of Glasgow email address, which you can request through the Ivanti help desk portal. </p> <p>The account will be bound to your University of Glasgow GUID, and therefore will have the same credentials and be disabled when you leave the organisation. </p> <p>We expect users to request their own account. A user account request should not be submitted by another person on behalf of that user. If you have a request for a bulk account creation of more than 10 accounts, please open a HPC Support request through Ivanti.</p> <p>In the case, where a formerly student user becomes a staff member with a new GUID they must request a new account through Ivanti. If the data of the old student account is still available, an admin may move the data over, upon request by the user.</p> <p>Use the University of Glasgow\u2019s self-service portal Ivanti to request your account. </p> <p>User Registration</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This manual will give you all the information you need to use a HPC cluster. You can find more guides in the additional content after this page, you will find more specialised guides, tutorial and references.</p>"},{"location":"quickstart/#introduction","title":"Introduction","text":"<p>HPC stands for High Performance Computing. Commonly this refers to a cluster of servers with resources shared by multiple people. To manage resource allocation a scheduler is used that, based on your definitions, creates an isolated work environment to run code. As the resource is shared and not always available, jobs can be queued for a while before they are run by the scheduler. No instant access to resources is guaranteed.</p>"},{"location":"quickstart/#how-can-hpc-help-you","title":"How can HPC help you?","text":"<ul> <li>Your work has outgrown your personal device's resources.</li> <li>Your work runs for hours or days and prevents you from using your personal device for other work.</li> <li>You do not have the right resources, like GPU, available to you in your personal device.</li> <li>You don't have the resources to acquire an own powerful computer and maintain it.</li> </ul>"},{"location":"quickstart/#architecture","title":"Architecture","text":"<p>HPC can be built in in different configurations. This configuration is very popular and you will see it in many HPCs:</p> <p></p> <p>The HPC is made up of these components:</p> <ul> <li>Login Node: This is the system all users interact with. If you want to use any of the other components, it has to be through the login node.</li> <li>Scheduler: This is the brain of the cluster. All your scheduling and status requests from the login node go to this system. </li> <li>Compute Servers: This symbolises all compute servers you could get allocations from through the scheduler. You should not access the servers directly unless you have an allocation. </li> <li>Shared Storage: Most of the storage you interact with on the cluster is shared between the login node and all available compute servers. Therefore you can manage the data you need within your jobs from the login node.</li> <li>HPC Network: The network is shut off from the Campus network and therefore systems within it, will not be accessible to users outside of the login node.</li> </ul>"},{"location":"quickstart/#how-does-hpc-work","title":"How does HPC work?","text":"<p>You will be submitting \"work packages\" in form of submission scripts, that define your resource requirement, an educated estimation of how long your job will run, and the workflow you want to run. Your work should run fully autonomous, this means no human interaction like GUI or console inputs. You can also have interactive sessions through HPC, these are useful for environment preparations, tests and debugging, but should be avoided for large production work. </p> <p>Your work package will be queued by the scheduler. The scheduler will decide where your job can run, based on your resource requirements described. Multiple jobs can run on one server. If there is enough resource available your job might run right away, otherwise it will be queued and run at a later time. You should be able to provide contact information, and the scheduler will keep you in the loop if your job has started, finished or failed. </p> <p></p> <p>Scheduling is a complicated matter, and multiple factors play into the priority of your job, however generally, the smaller your job, the faster it will run, so it pays out to be efficient!</p>"},{"location":"quickstart/#access-the-cluster","title":"Access the Cluster","text":"<p>After you got your account, you can log into the login node. The login node is the central point of access to the cluster for all users. This server is not very powerful and should therefore not be used for computational work. Any computational work should go through a job allocation on the scheduler.</p> <p>Info</p> <p>You have to be connected to the Campus network either via LAN, eduroam or VPN to access the cluster.</p>"},{"location":"quickstart/#connection-information","title":"Connection Information","text":"Lochan <ul> <li>Hostname: <code>lochan.hpc.gla.ac.uk</code></li> <li>Username: University of Glasgow GUID</li> <li>Password: GUID Password</li> </ul>"},{"location":"quickstart/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>You will need to use <code>SSH</code> to connect to the login node and use the HPC. The simplest way to connect is by opening a console and connect using the preinstalled <code>SSH</code> utility of your device (If you are prompted for a password, it will not show up while typing):</p> <pre><code>ssh &lt;username&gt;@&lt;hostaname&gt;\n</code></pre> <p>We would recommend you use a SSH GUI client for regular access to the platform, as it allows you to save sessions, and <code>copy+paste</code> more easily. Example software are PuTTY and MobaXterm, however you can use whatever you prefer. </p>"},{"location":"quickstart/#data-management","title":"Data Management","text":"<p>Data is an important part of HPC. Where and how to store your data is important for efficient usage of the platform. </p> <p>All storage available is to be used for the duration of your work. It is not expected to provide long term/primary storage. The data will assumed to be transient with only limited protection. As the HPC is not a primary storage solution, we recommend storing all HPC data, you can\u2019t afford to lose in a primary, safe location like a centralised storage system provided by your school or a Team within Microsoft Teams. </p>"},{"location":"quickstart/#storage-spaces","title":"Storage Spaces","text":"Lochan <p>Warning</p> <p>This is not a trusted research environment, therefore all research data must be anonymised prior to transferring it onto the system.</p> <p>User Home</p> Size 100G (quota per user) Path <code>/mnt/home/&lt;GUID&gt;</code> Use Set up your environments and store all the scripts and data you need for your personal use. <p>Shared User Scratch</p> Size 280Tb (shared between all cluster users) Path <code>~/sharedscratch</code> or <code>/mnt/scratch/users/&lt;GUID&gt;</code> Use This storage is shared between all nodes. Read and write data that you need during your jobs. Please ensure to clean up your scratch space after you are done processing your job, to make the space available for other users to use!"},{"location":"quickstart/#transfer-data","title":"Transfer Data","text":"<p>To transfer data from your local machine (or another system), you can use <code>SSH</code>. You can do this either with the <code>scp</code> command:</p> <pre><code>scp &lt;source file&gt; &lt;guid&gt;@&lt;hostname&gt;:&lt;target file&gt;\n</code></pre> <p>Or you can use a graphical SFTP Client of choice, for example WinSCP. Use the connection details of the login node, mentioned above to connect.</p>"},{"location":"quickstart/#scheduler","title":"Scheduler","text":"<p>The scheduler used is Slurm Workload Manager, developed by SchedMD. Slurm has a very in depth documentation themselves, which could be useful to read through, for a more in depth understanding of how this software works Quick Start User Guide. </p> <p>The information here describes configurations you will need to know to use the specific cluster.</p>"},{"location":"quickstart/#resources","title":"Resources","text":"<p>Compute servers - also referred to as nodes - can carry different resource configurations to fit different workloads. For example, some servers might offer high amount of CPU, while others offer GPU resource.</p> Lochan <p>Lochan is very heterogeneous, meaning it is comprised of a vast variety of hardware! You can  get an overview of all servers and their available resources by running the command below on  the system:</p> <pre><code>sinfo -o \"%20n %10c %20m %30G\"\n</code></pre> Explanation of command output <ul> <li>CPUS: Number of CPUs available on the node.</li> <li>MEMORY: Amount of memory / RAM available on the node in MB.</li> <li>GRES: GPU resources available on the node. <code>gpu:&lt;type&gt;:&lt;amount&gt;</code>.</li> </ul>"},{"location":"quickstart/#partitions-queues","title":"Partitions / Queues","text":"<p>Partitions, also known as queues on other scheduling systems, are used to determine which nodes you want your job to run. To see the partition configuration of the HPC you are using run this command:</p> <pre><code>scontrol show partition\n</code></pre> Lochan Partition Description cpu This is the default partition, meaning this is chosen when no partition is specified. It contains all CPU focused servers of the Cluster. gpu This partitions contains all servers with GPU resources available. You can specify which type with the <code>--gres</code> parameter."},{"location":"quickstart/#software","title":"Software","text":"Lochan <p>On Lochan, users are responsible for their own software. Though we offer some tools through modules, that make it easier for users to manage their own software. We also offer modules for software, that have a shared license, that is managed by the University of Glasgow.</p>"},{"location":"quickstart/#environment-modules","title":"Environment Modules","text":"<p>This is software that is centrally installed and can be used across the cluster. The full manual of <code>modules</code> can be found here.</p> <p>The most used commands are listed below:</p> Command Description <code>module available</code> List all available modules on the system. <code>module load</code> Activate module for use in your current session. <code>module unload &lt;name&gt;</code> Deactivate module from your current session. <code>module search &lt;search_term&gt;</code> Search for modules by name or description. <code>module list</code> List all active modules in your current session. <code>module purge</code> Deactivate all modules loaded in your current session."},{"location":"quickstart/#support","title":"Support","text":"<p>The RCaaS HPC Admin team is happy to help where possible, no matter if you are a novice or experienced user. Feel free to get in touch! All our services can be found through Ivanti:</p> <p>Ivanti Help Desk</p>"},{"location":"clusters/lochan/","title":"Lochan HPC Cluster","text":"<p>Information Services run a High Performance Compute (HPC) cluster called Lochan, that is available to use by all staff, students and affiliates of the University of Glasgow.</p> <p>The platform and team around it have been supporting researchers in all colleges since 2014. Managed by Research Computing as a Service (RCaaS) and motivated by the needs of the community, Lochan is here for people with large upscaling needs but also users who are just starting their HPC journey.</p>"},{"location":"clusters/lochan/#technology","title":"Technology","text":"<p>Lochan offers the following compute resources:</p> <ul> <li>856 virtual CPUs</li> <li>3 NVIDIA Tesla V100 GPUs</li> <li>2 Nvidia L40S GPUs</li> <li>8 Nvidia GeForce GTX 1080 GPUs</li> <li>14+TB GB of memory with up to 2TB per node</li> </ul> <p>All servers of the cluster run on Oracle Linux 9. Oracle Linux (OL) is a Red Hat based open-source operating system.</p> <p>The cluster's scheduler is the Slurm Workload Manager, developed by SchedMD. This software is crucial for HPC and helps achieve fair usage of all available compute resources.</p> <p>The cluster and storage mounted on to the system are located in Saughfield House on the University of Glasgow Campus next to the Library.</p>"},{"location":"clusters/lochan/#acknowlegement","title":"Acknowlegement","text":"<p>Where Lochan is used in the development of research outputs the following attribution should be used:</p> <p>This research utilised the University of Glasgow\u2019s Lochan HPC, supported by University of Glasgow Research Computing as a Service. (https://www.gla.ac.uk/myglasgow/it/hpcc/lochan/)</p>"},{"location":"guides/apptainer/","title":"Apptainer","text":"<p>This guide will help you familiarise yourself with Apptainer (formerly known as Singularity) and some of its utilities. This can hopefully kickstart you to the usage of images (containers) for easily reproducible work.</p> <p>For more advanced information, please consult the official documentation of the application: </p> <ul> <li>Apptainer Documentation</li> </ul>"},{"location":"guides/apptainer/#run-images","title":"Run images","text":"<p>You can run images you made yourself, or run images provided by colleagues or software developers. Please make sure the images you are using are from trusted sources!</p> <p>You can run a container using the <code>apptainer run</code> command. The <code>&lt;image&gt;</code> part can either be the path to a <code>.sif</code> file you have saved locally or it can be a link to an image stored in a repository online.</p> <pre><code>apptainer run &lt;options&gt; &lt;image&gt;\n</code></pre> <p>As an example we will use the image hosted on the GitHub container registry by the Apptainer developers: <code>docker://ghcr.io/apptainer/lolcow</code>. All you need to do is provide the link and Apptainer will run the %runscript code of the image. In our example this will be a cow, telling you the current time and date:</p> <pre><code>$ apptainer run docker://ghcr.io/apptainer/lolcow\n ________________________\n&lt; Tue Feb 11 14:02:52 GMT 2025 &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>If you want to run a command outside of the workflow defined in <code>%runscript</code>, you can use the <code>apptainer</code> exec command:</p> <pre><code>apptainer exec &lt;options&gt; &lt;image&gt; &lt;command&gt;\n</code></pre> <p>This will not run the code within <code>%runscript</code> and instead run the code you provide on the command line:</p> <pre><code>$ apptainer exec docker://ghcr.io/apptainer/lolcow echo \"I won't do what I was made for!\"\nI won't do what I was made for!\n$\n</code></pre> <p>If you want to test and debug, you can run an interactive shell from within a container, using the <code>apptainer shell</code> command. </p> <pre><code>apptainer shell &lt;options&gt; &lt;image&gt;\n</code></pre> <p>To leave just use exit:</p> <pre><code>$ apptainer shell docker://ghcr.io/apptainer/lolcow\nApptainer&gt; echo $SHELL\n/bin/bash\nApptainer&gt; echo \"Hello, world!\"\nHello, world!\nApptainer&gt; exit\n$\n</code></pre> <p>All three of these commands have similar parameters. If you want to see all available options for the command run <code>man apptainer &lt;run/exec/shell&gt;</code> in your console. Below we explain some options we see as important:</p> Option Template Example Description <code>-B, --bind=[]</code> <code>-B &lt;src&gt;:&lt;dest&gt;</code> <code>-B ~/mydata:/datadir</code> Binds the local directory <code>~/mydata</code> to <code>/datadir</code> within the container. <code>--nv</code> <code>--nv</code> <code>--nv</code> Enables Nvidia support. Use this option when working on GPU-servers to ensure you have the GPU available within your container."},{"location":"guides/apptainer/#create-images","title":"Create images","text":"<p>You won't always be able to create images on the HPC, we recommend you install apptainer on your local device. Information on how to install can be found here.</p> <p>Once you have an installation of apptainer working on your device, you can build images to then use on a HPC or any other device. For our simple example, we\u2019ll build an image to install and run the test package <code>hello</code> from the Ubuntu upstream repository.</p> <p>All we need is to create a Definition File outlining which OS we want to make use (<code>#Header</code>), install the packages we want (<code>%post</code>) and the action to run (<code>%runscript</code>). We will call this file <code>hello.def</code>:</p> <pre><code>#Header\nBootStrap: docker\nFrom: ubuntu:latest\n\n%post\n    apt-get update\n    apt-get -y install hello\n    apt-get clean\n\n%runscript\n    hello\n</code></pre> <p>We can then create a image file (<code>.sif</code>) from that definition file. The <code>.sif</code> file will be the image, from which our container will be started. We can move and run it on other systems like an HPC. We will call our image <code>hello.sif</code>:</p> <pre><code>apptainer build hello.sif hello.def\n</code></pre> <p>If we copy <code>hello.sif</code> over to an HPC for example, we can now run it using apptainer:</p> <pre><code>$ hello\n-bash: hello: command not found\n$ apptainer run hello.sif\nHello, world!\n</code></pre>"},{"location":"guides/conda/","title":"Conda","text":"<p>This guide will give you some helpful information on how to use <code>conda</code> to manage different software environments on HPC. The HPC admin team recommends to use Miniforge as your preferred conda installation.</p> <p>For more advanced information, please consult the official documentation of the application:</p> <ul> <li>Conda Documentation</li> </ul>"},{"location":"guides/conda/#create-environment","title":"Create environment","text":"<p>Create your environment and give it a fitting name, for example the project you are working on or the software you are intending to use with it.</p> <pre><code>conda create -n demo-env \n</code></pre> <p>Next activate your environment. This should add a prefix to your prompt with your environment name chosen above.</p> <pre><code>conda activate demo-env \n</code></pre> <p>Now you can install software to be available within your environment. The syntax is <code>&lt;software-name&gt;=&lt;software-version&gt;</code>. If you don\u2019t provide a version, the latest will be pulled.</p> <pre><code>conda install python=3.9 \n</code></pre> <p>All changes you make while in the environment will be saved to it. So, if you deactivate, they will no longer be available. In turn, they will be available the same you left it, when you reactivate.</p>"},{"location":"guides/conda/#list-and-export-environment","title":"List and export environment","text":"<p>To get a list of all available environments use the command below. In the list you can also find where the environment is stored. This means all packages and libraries are within that sub-path.</p> <pre><code>conda env list\n</code></pre> <p>To get a list of all installed packages within an environment use this command</p> <pre><code>conda list -n demo-env\n</code></pre> <p>You can also create an export for your environment:</p> <pre><code>conda env export &gt; demo-env.yml\n</code></pre> <p>You can now move this <code>.yml</code> file onto another system or share it with a colleague, where they can rebuild your environment using the below command:</p> <pre><code>conda env create -f demo-env.yml\n</code></pre>"},{"location":"guides/conda/#delete-environment","title":"Delete environment","text":"<p>Conda environments can grow large and its important to keep your workspace clean. Therefore, make sure to delete old environments you don\u2019t use anymore.</p> <pre><code>conda env remove -n demo-env\n</code></pre>"},{"location":"guides/conda/#set-up-diffrent-environment-location","title":"Set up diffrent environment location","text":"<p>Info</p> <p>The paths and names used here are examples. Please adjust them to what fits for the environment you are working in</p> <p>It could be, that for quota reasons, or to be able to share environments with colleagues, you want to have environments in a different location, than the default <code>~/.conda/envs</code>.</p> <p>For this create a directory in a different location, for your environments to sit in:</p> <pre><code>mkdir /mnt/myproject/software/conda-envs\n</code></pre> <p>You can now add this directory to your config, so <code>conda</code> reads it as a new location to fetch environments from:</p> <pre><code>conda config --add envs_dirs /mnt/myproject/software/conda-envs\n</code></pre> <p>To verify your configuration is correct run:</p> <pre><code>conda config --show\n</code></pre> <p>Note</p> <p>The directory shown at the top of the list is the default. The directory last added will be the new default. If you wanted to change that, adjust the order in the file <code>~/.condarc</code>.</p> <p>With this config you might get very long shell prompts showing the whole path to the environment. To avoid that run:</p> <pre><code>conda config --set env_prompt '({name})'\n</code></pre>"},{"location":"guides/jupyter/","title":"Jupyter","text":"<p>This guide will help you how to use a jupyter server through HPC.</p> <p>For more advanced information, please consult the official documentation of the application: </p> <ul> <li>Jupyter Documentation</li> </ul>"},{"location":"guides/jupyter/#install-jupyter","title":"Install Jupyter","text":"<p>Jupyter can easily be installed by yourself using <code>conda</code>. The manual for <code>conda</code> can be found here. However you are free to use any other installation method you prefer for this. Below is a step by step manual on how to create a new environment with jupyter installed.</p> <p>First we create a new environment:</p> <pre><code>conda create -n jupyter-env\n</code></pre> <p>Once that is through, we can activate our new environment:</p> <pre><code>conda activate jupyter-env\n</code></pre> <p>Now we can install jupyter and any other packages, we may need into this environment:</p> <pre><code>conda install jupyter\n</code></pre>"},{"location":"guides/jupyter/#use-jupyter-on-hpc","title":"Use Jupyter on HPC","text":"<p>Using Jupyter server is a quite inconvenient way to schedule your work, and we would recommend where possible to run your code directly without the interactive use of the server.</p> <p>First, you should establish where your jupyter working directory is. This is where all your scripts you intend to run are. In the case, you are starting from scratch, we will create one:</p> <pre><code>mkdir ~/my-jupyter-work-dir\n</code></pre> <p>To standardise access to the Jupyter server without a randomly created access token, we\u2019ll set a password. This removes the need to remember the generated token later on. To do this use the command below and enter a password (remember that on UNIX, you won\u2019t see password inputs).</p> <pre><code>jupyter server password\n</code></pre> <p>Please be sure to save your password in a password manager, so you don\u2019t lose it. To change your password just run the command again. This will overwrite the old one. The password is saved as a hash into <code>$HOME/.jupyter/jupyter_server_config.json</code>. If you want to remove the password configuration, you can delete it from the file.</p> <p>When starting the jupyter server, we will be using three parameters. They ara the following:</p> Parameter Value Note <code>--no-browser</code> This starts a jupyter server, without trying to open a browser. <code>--ip</code> <code>0.0.0.0</code> This makes, so the jupyter server listens from external sources, in our case this is the login-node. <code>--port</code> &lt;5 digit number&gt; Port under which your server is available. This should be unique on the cluster, so choose 5 digits at random, but not higher than 65535. <p>The jupyter command would look something like this. As mentioned above, please ensure to customise your ==port==. If you get an error, try changing the port and start again.</p> <pre><code>jupyter server --no-browser --ip=0.0.0.0 --port=12365\n</code></pre> <p>We will now package this in an easily reusable batch submission script. For this choose a template of your chose from here, and add the following lines of code to the bottom:</p> <pre><code>#Load any modules or activate any environment you need to run Jupyter\nconda activate jupyter-env\n\n#Move to your jupyter working directory\ncd ~/my-jupyter-work-dir\n\n#Start your Jupyter server\njupyter server --no-browser --ip=0.0.0.0 --port=12365\n</code></pre> <p>We can now submit this via the <code>sbatch</code> utility:</p> <pre><code>sbatch myJupyterServerJob.sh\n</code></pre> <p>We will now need to figure our the ==nodeName==, where our Jupyter server is running from. You can do that using <code>squeue</code> and either using the JobID or your GUID. The <code>nodeName</code> is the value below <code>NODELIST</code>:</p> <p>Info</p> <p>Ensure that <code>ST</code> (status) is <code>R</code> (running). If the cluster is in high demand, you might not get your allocation right away!</p> <pre><code>$ squeue -j &lt;JobID&gt;\nJOBID    PARTITION    NAME      USER    ST   TIME  NODES  NODELIST(REASON)\n&lt;JobID&gt;  nodes        Jupyter-  &lt;GUID&gt;  R    0:14      1  node05\n$ squeue -u &lt;GUID&gt;\nJOBID    PARTITION    NAME      USER    ST   TIME  NODES  NODELIST(REASON)\n&lt;JobID&gt;  nodes        Jupyter-  &lt;GUID&gt;  R    0:44      1  node05\n</code></pre> <p>We will now setup a SSH port forwarding to the HPC login node, so we can seamlessly use our local browser, to work with the server. To do this, open a command line from your local device, and enter the command below, replacing <code>&lt;port&gt;</code>, <code>&lt;nodeName&gt;</code> and <code>&lt;GUID&gt;</code> with the values we saved.</p> <pre><code>ssh -L &lt;port&gt;:&lt;nodeName&gt;:&lt;port&gt; &lt;GUID&gt;@mars-login.ice.gla.ac.uk\n</code></pre> <p>You should be asked for your password and logged into the HPC, however you can ignore this console from now on and minimise it.</p> <p>Now you can start a browser of your choice on your local device and navigate to this link and customise <code>&lt;port&gt;</code> it with your port: <code>http://localhost:&lt;port&gt;/tree?</code></p> <p>To log into the server, you can use the password you set earlier.</p> <p>After you are done with your work, you can close the browser, and exit the session in your minimised console, you used for the SSH port forwarding. If you finish your work, before your jupyter server job is done running, please kill it by using <code>scancel</code>, followed by the JobID:</p> <pre><code>scancel &lt;JobID&gt;\n</code></pre> <p>Warning</p> <p>Jupyter servers, that are found to be running for long periods of time on idle nodes might be killed by HPC administrators. Leaving an unused Jupyter server running is wasting resources other users may be waiting to use on the cluster!</p>"},{"location":"guides/parallel/","title":"Parallel","text":"<p>If you want to run code in parallel, HPC is perfect, as you have a lot of resource available, to efficiently scale your work!</p> <p>Important</p> <p>The software or code you are using has to be capable to run in parallel. Not every software can do that and code has to be specifically written for it!</p>"},{"location":"guides/parallel/#automatic-scaling-from-slurm-allocation","title":"Automatic scaling from Slurm allocation","text":"<p>Some programs, will try to use all CPU a node (server) has available, when trying to run in parallel, but depending on your submission, you won't always have all CPUs per server available.</p> <p>If you want to easily adjust your parallel program, to spawn processes based on your submission, you can do that using Slurm environment variables, set in every submission.</p> Variable Explanation <code>SLURM_NNODES</code> Amount of nodes (servers) your job is running on. <code>SLURM_CPUS_ON_NODE</code> Amount of CPU available per node (server). <code>SLURM_NTASKS</code> Amount of tasks available per server (server). <p>Here an example with a simple parallel processing python script. The script creates eight processes that each sleep for four seconds:</p> <pre><code>import multiprocessing\nimport time\nimport os\n\nslurm_threads = os.environ['SLURM_CPUS_ON_NODE']\n\n\ndata = ('A','B','C','D','E','F','G','H')\n\ndef sleeper(id):\n    print(\"Processs %s started\" % id)\n    time.sleep(4)\n    print(\"Process %s finished\" % id)\n\ndef mp_handler():\n    pool = multiprocessing.Pool(int(slurm_threads))\n    pool.map(sleeper, data)\n\nif __name__ == '__main__':\n    mp_handler()\n</code></pre> <p>By saving the value of the environment variable <code>SLURM_CPUS_ON_NODE</code> into the python variable <code>slurm_threads</code>, we won't need to touch our python script anymore to upscale our work, but rather just adjust our submission. </p> <p>This run will take 32 seconds:</p> <pre><code>srun --cpus-per-task=1 --pty python3 ~/parallel.py\n</code></pre> <p>This run will take 8 seconds:</p> <pre><code>srun --cpus-per-task=4 --pty python3 ~/parallel.py\n</code></pre> <p>This run will take 4 seconds:</p> <pre><code>srun --cpus-per-task=8 --pty python3 ~/parallel.py\n</code></pre>"},{"location":"guides/parallel/#openmpi-example","title":"OpenMPI Example","text":"<p>The example source code is written in C below and saved to a file called <code>hello_mpi.c</code>:</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;unistd.h&gt;\n\nint main (int argc, char *argv[])\n{\n        int threadid,totalthreads;\n        char hostname[80];\n\n        // Start MPI processes on each node\n        MPI_Init(&amp;argc,&amp;argv);\n\n        // Get thread ID from the MPI master process\n        MPI_Comm_rank(MPI_COMM_WORLD, &amp;threadid);\n\n        // Get the number of processes launched by MPI\n        MPI_Comm_size(MPI_COMM_WORLD, &amp;totalthreads);\n\n        // Get the hostname from the node running the task\n        gethostname(hostname,80);\n\n        // Print a statement to standard output\n        printf(\"Hello from process %i on machine %s, of %i processes\\n\", threadid, hostname, totalthreads);\n\n\n        MPI_Finalize();\n\n        return 0;\n}\n</code></pre> <p>Compile the code above into an executable called <code>hello_mpi</code>:</p> <pre><code>mpicc hello_mpi.c -o hello_mpi\n</code></pre> <p>You can now write a submission script to run your MPI program <code>hello_mpi</code> over multiple nodes. First you can copy a template from here and adjust as described below.</p> <p>Adjust the <code>--nodes</code> to <code>2</code> to run the job across two servers and the <code>--ntasks-per-node</code> to <code>4</code> to spawn four processes per node, so eight in total. MPI will automatically spawn as many processes, as tasks are available in the allocation.</p> <pre><code>...\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n...\n</code></pre> <p>Run the program compiled with the <code>mpirun</code> utility: <pre><code>...\n############# MY CODE #############\nmpirun hello_mpi\n</code></pre></p> <p>Your output file will contain something like this:</p> <pre><code>Hello from process 0 on machine node001, of 8 processes\nHello from process 1 on machine node001, of 8 processes\nHello from process 2 on machine node001, of 8 processes\nHello from process 3 on machine node001, of 8 processes\nHello from process 6 on machine node002, of 8 processes\nHello from process 7 on machine node002, of 8 processes\nHello from process 4 on machine node002, of 8 processes\nHello from process 5 on machine node002, of 8 processes\n</code></pre>"},{"location":"guides/r/","title":"R","text":"<p>This guide will help you familiarise yourself with R on HPC.</p> <p>For more advanced information, please consult the official documentation of the application: </p> <ul> <li>R Documentation</li> </ul>"},{"location":"guides/r/#run-code-autonomous","title":"Run code autonomous","text":"<p>The easiest way to run your R code on HPC is to save your code in a file and run it with the <code>Rscript</code> utility.</p> <p>Here a very simple example R script file, it's named <code>myRCode.R</code>:</p> <pre><code>print(\"Hello World!\")\n</code></pre> <p>Now if you want to execute this within your Slurm job submission, just use one of our templates here and edit it as described below. The example submission script is named <code>mySlurmJob.sh</code>.</p> <p>In our submission script, we will now run the code autonomously via <code>Rscript</code> by supplying it as a simple parameter:</p> <pre><code>...\n############# MY CODE #############\nRscript myRCode.R\n</code></pre> <p>Now we can just submit our submission script with the Slurm <code>sbatch</code> utility:</p> <pre><code>sbatch mySlurmJob.sh\n</code></pre> <p>You will find the generated console output in the file you specified in the <code>--output</code> parameter within your Slurm submission.</p>"},{"location":"guides/r/#install-packages-in-custom-space","title":"Install packages in custom space","text":"<p>Per default, your downloaded R packages are saved within your <code>$HOME</code> directory in <code>.R/&lt;version&gt;</code>. If you are running out of space, you can move the installation path into a different directory.</p> <p>Info</p> <p>The paths and names used here are examples. Please adjust them to what fits for the environment you are working with.</p> <p>Create a software directory:</p> <pre><code>mkdir -p /mnt/myproject/software/R\n</code></pre> <p>Install packages into the new directory within <code>R</code>:</p> <pre><code>install.packages(\"&lt;package-name&gt;\", lib=\"/mnt/myproject/software/R\")\n</code></pre> <p>Add the new directory to <code>$R_LIBS_USER</code> variable. With this set, R will expand its software search to the directory specified:</p> <pre><code>export R_LIBS_USER=\"/mnt/myproject/software/R:$R_LIBS_USER\"\n</code></pre> <p><code>$R_LIBS_USER</code> has to be set like this whenever you want to use the software in your custom directory. You can either do this at the beginning of your job, in your submission script or add it to your <code>~/.bashrc</code> profile script, to make it available at every login.</p>"},{"location":"guides/rclone/","title":"Rclone","text":"<p>Rclone is a command-line program to manage files on cloud storage. Over 70 cloud storage products support Rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>For more advanced information, please consult the official documentation of the application: </p> <ul> <li>Rclone Documentation</li> </ul>"},{"location":"guides/rclone/#encrypt-configuration","title":"Encrypt configuration","text":"<p>To keep all your information safe, you should encrypt your rclone configuration. From there on, you will have to use a password to use rclone, so be sure to store your password in a safe place!</p> <p>Enter configuration manager:</p> <pre><code>rclone config\n</code></pre> <p>Enter <code>s</code> to set configuration password and confirm by entering <code>a</code>:</p> <pre><code>No remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; s\nYour configuration is not encrypted.\nIf you add a password, you will protect your login information to cloud services.\na) Add Password\nq) Quit to main menu\na/q&gt; a\n</code></pre> <p>Enter your password. Keep in mind, that the password will not be shown in the console:</p> <pre><code>Enter NEW configuration password:\npassword: &lt;your password won\u2019t show when entered&gt;\nConfirm NEW configuration password:\npassword: &lt;your password won\u2019t show when entered&gt;\nPassword set\n</code></pre> <p>You can now leave the configuration manager by entering <code>q</code>:</p> <pre><code>Your configuration is encrypted.\nc) Change Password\nu) Unencrypt configuration\nq) Quit to main menu\nc/u/q&gt; q\n</code></pre>"},{"location":"guides/rclone/#setup-for-teams-storage","title":"Setup for Teams storage","text":"<p>As Teams / OneDrive / SharePoint is the University of Glasgow\u2019s official cloud storage, we\u2019ll walk through on how to configure a remote connection to a Microsoft Team storage through <code>rclone</code>. Manuals for other connections can be found in the official documentation of Rclone.</p> <p>Enter configuration manager:</p> <pre><code>rclone config\n</code></pre> <p>Create a new remote connection by entering <code>n</code>:</p> <pre><code>No remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\n</code></pre> <p>Choose a recognisable name for your remote connection. Something like \"Teams_TeamName\":</p> <pre><code>Enter name for new remote.\nname&gt; Teams_HPC-Admin\n</code></pre> <p>After you will get a list of connection types. For Teams use \"Microsoft OneDrive\" by entering <code>onedrive</code>:</p> <pre><code>\u2026\n26 / Microsoft OneDrive\n   \\ (onedrive)\n\u2026\nStorage&gt; onedrive\n</code></pre> <p>For the next four steps just continue by pressing \"return\" and choosing the default option:</p> <pre><code>Option client_id.\nOAuth Client Id.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_id&gt;\n\nOption client_secret.\nOAuth Client Secret.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_secret&gt;\n\nOption region.\nChoose national cloud region for OneDrive.\nChoose a number from below, or type in your own string value.\nPress Enter for the default (global).\n 1 / Microsoft Cloud Global\n   \\ (global)\n 2 / Microsoft Cloud for US Government\n   \\ (us)\n 3 / Microsoft Cloud Germany\n   \\ (de)\n 4 / Azure and Office 365 operated by Vnet Group in China\n   \\ (cn)\nregion&gt;\n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt;\n</code></pre> <p>To create your authentication token, we use the browser on your personal device, so enter <code>n</code>:</p> <pre><code>Use web browser to automatically authenticate rclone with remote?\n * Say Y if the machine running rclone has a web browser you can use\n * Say N if running rclone on a (remote) machine without web browser access\nIf not sure try Y. If Y failed, try N.\n\ny) Yes (default)\nn) No\ny/n&gt; n\n</code></pre> <p>On your local laptop, download rclone from the official website: Download.</p> <p>Extract the downloaded .zip archive and start a shell from that directory. On Windows you can do it easily by clicking into the navigation, typing <code>cmd</code> and pressing \"return\".</p> <p>Within your local console run the following command:</p> <pre><code>rclone authorize \"onedrive\"\n</code></pre> <p>This will open a browser on your device. If you have a UofG managed device, you should be logged in automatically. If not, log into your UofG account (GUID).</p> <p>After you give Rclone the required permissions on your account, you should get a \"Success!\" message in the browser. Switch back to your local console and copy the <code>access_token</code> that was printed into the console.</p> <p>You can now close your local console and delete the Rclone files from your laptop, if you don\u2019t plan to set up more remote connections.</p> <p>Back in your HPC console, paste the access_token and press \"return\":</p> <pre><code>For this to work, you will need rclone available on a machine that has\na web browser available.\n\nFor more help and alternate methods see: https://rclone.org/remote_setup/\n\nExecute the following on the machine with the web browser (same rclone\nversion recommended):\n\n        rclone authorize \"onedrive\"\n\nThen paste the result below:\nresult&gt; {\"access_token\":\"&lt;string of random\ncharacters&gt;\",\"expiry\":\"2024-12-16T14:59:27.6563422+01:00\"}\n</code></pre> <p>If you are getting an error that looks along the lines of \"<code>Fatal error: failed to configure OneDrive: empty token found</code>\", run the following command below in your console and start the configuration again:</p> <pre><code>sttyback=$(stty -g) ; stty -icanon\n</code></pre> <p>Careful, with this command active, you are no longer able to use backspace to correct text in the Rclone interactive setup, so ensure you are typing or pasting things correctly! Once you finish the configuration, you can either restart your shell, to deactivate this command, or run:</p> <pre><code>stty $sttyback\n</code></pre> <p>After the token has been accepted, you can choose \"SharePoint site name or URL\" from the list:</p> <pre><code>... \n 3 / Sharepoint site name or URL (e.g. mysite or https://contoso.sharepoint.com/sites/mysite)\n   \\ \"url\"\n...\nYour choice&gt; url\n</code></pre> <p>Copy the URL of your Team. You can access it, when you open your Teams storage in SharePoint (web). It should look something like \"https://gla.sharepoint.com/sites/TeamName\". Enter this URL in the Rclone config dialog:</p> <pre><code>Option config_site_url.\nSite URL\nExample: \"https://contoso.sharepoint.com/sites/mysite\" or \"mysite\"\nEnter a value.\nconfig_site_url&gt; https://gla.sharepoint.com/sites/MARSAdmin\n</code></pre> <p>It will show you the drives it found on the Team. Select the one called Documents by entering <code>0</code> and confirming with <code>y</code>:</p> <pre><code>Found 1 drives, please select the one you want to use:\n0: Documents (documentLibrary) id=&lt;string of random characters&gt;\nChose drive to use:&gt; 0\nFound drive \"root\" of type \"documentLibrary\"\nURL: https://gla.sharepoint.com/sites/MARSAdmin/Shared%20Documents\nIs that okay?\ny) Yes (default)\nn) No\ny/n&gt; y\n</code></pre> <p>You will then get a summary of the configuration you just made. Review it and save it by pressing \"return\"\" again:</p> <pre><code>&lt;your config&gt;\nKeep this \"Team_MARS-Admin\" remote?\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt;\n</code></pre> <p>You can now use your configured remote connection in rclone commands as either a source or destination. The syntax is as follows:</p> <pre><code>&lt;remote-name&gt;:&lt;path&gt;\n</code></pre> <ul> <li><code>&lt;remote-name&gt;</code>: The name given to your remote connection in the first step of the configuration process</li> <li><code>&lt;path&gt;</code>: The path to the directory in SharePoint where you want to save your data.</li> </ul>"},{"location":"guides/rclone/#move-data-from-and-to-storage","title":"Move data from and to storage","text":"<p>The command to move data is:</p> <pre><code>rclone move &lt;source&gt; &lt;destination&gt;\n</code></pre> <p>In this example we move data from the HPC to the Teams storage previously configured. The directories specified will be created on the Teams storage if they don\u2019t already exist.</p> <pre><code>rclone move ~/myResearchResults Teams_HPC-Admin:Results/2024-05-01\n</code></pre> <p>All the files in your source directory will be moved and you should see them your SharePoint site.</p> <p>To move data back to the HPC, just switch source and destination:</p> <pre><code>rclone move Teams_HPC-Admin:Results/2024-05-01 ~/myResearchResults\n</code></pre> <p>If you feel uncomfortable, with your data being gone from the source, use <code>copy</code> instead of <code>move</code>.</p>"},{"location":"guides/rclone/#backing-up-data","title":"Backing up data","text":"<p>It is important to keep your data on a secondary system, preferably a highly available one like OneDrive. The sync command makes source and destination identical, modifying the destination only!</p> <p>The command to sync your directory with a remote storage is:</p> <pre><code>rclone sync &lt;source&gt; &lt;destination&gt;\n</code></pre> <p>In this example we backup data from the HPC to the Teams storage previously configured. The directories specified will be created on the Teams storage if they don\u2019t already exist.</p> <pre><code>rclone sync ~/myData Teams_HPC-Admin:Backup/HPC-Data\n</code></pre>"},{"location":"guides/slurm/","title":"Slurm","text":"<p>This guide will help you familiarise yourself with Slurm and its utilities. It will give you helpful code snippets you can use on all Slurm systems you interact with.</p> <p>For more advanced information, please consult the official documentation of the application: </p> <ul> <li>Slurm Documentation</li> </ul>"},{"location":"guides/slurm/#get-cluster-information","title":"Get Cluster Information","text":"<p>In this section we will show you how to get information about the scheduler through different slurm queries. </p>"},{"location":"guides/slurm/#resources","title":"Resources","text":"<p>This command will show you an output of all servers  in the cluster with information about the amount or CPUs, amount of Memory (in MB) and if and what kind of GPU is available: </p> <pre><code>sinfo -o \"%20n %10c %20m %30G\"\n</code></pre>"},{"location":"guides/slurm/#partitions","title":"Partitions","text":"<p>To get an overview of all available partitions, including their state, node count, and CPU + time restrictions use this command:</p> <pre><code>sinfo -o \"%20P %10a %10D %20B %20l \"\n</code></pre> <p>If you want in in depth overview of a specific partition, you can use <code>scontrol</code>. This output is more technical:</p> <pre><code>scontrol show partition &lt;partition_name&gt;\n</code></pre>"},{"location":"guides/slurm/#usage","title":"Usage","text":"<p>If you want to get an overview of the usage of the platform, you can use the following command. The letters A/I/O/T stands for Allocated/Idle/Other/Total. Other usually means resource is down.</p> <pre><code>sinfo -O partition,nodelist,cpusstate,gres,gresused,nodeai\n</code></pre> <p>RCaaS has a script on all their managed systems called <code>load</code>, that shows you the usage of the platform per resource. You can either choose <code>cpu</code>, <code>gpu</code> or <code>mem</code>. The output is a bar visualising the usage percentage of the resource defined. </p> <pre><code>load &lt;resource&gt;\n</code></pre>"},{"location":"guides/slurm/#interactive-job","title":"Interactive Job","text":"<p>Interactive jobs are great to install software, prepare your environment or debug your script. For any serious or larger work, please use batch jobs.</p> <p>Get an allocation with <code>srun</code> and open an interactive bash shell with the parameter <code>--pty bash</code>. You can use any other slurm parameters, to define your allocation, but this parameter has to be the last one. If you require access to a GUI, you can use the <code>--x11</code> parameter.</p> <pre><code>srun &lt;slurm_parameters&gt; --pty bash\n</code></pre> <p>You are not guarateed an allocation right away! If the cluster is busy, you might be waiting hours for your interactive shell to start.</p> <p>If due to a network issue you lose access to this console, there is no way to get access back, this is why it is a bad idea to use it for longer running jobs! If you lose access and the job is still scheduled to run for a long amount of time, please kill it, to make the resource available for other users again.</p>"},{"location":"guides/slurm/#batch-job-submission","title":"Batch Job Submission","text":"<p>Batch job submission is the way the cluster is ideally used. You use a \"Submission Script\" to submit your work to the cluster. This script is usually a <code>bash</code> script containing your job specification, environment setup and then your work. Example scripts can  be found here Submission Script Templates.</p> <p>In this example here, we are creating a directory for our job, move into it and run a python  script:</p> <pre><code>#!/bin/bash -l\n...\n############# MY CODE ############# \nmkdir ~/work/job_$SLURM_JOBID\ncd ~/work/job_$SLURM_JOBID\npython3 myPYthonCalcs.py\n</code></pre> <p>To submit this to the scheduler we use the <code>sbatch</code> utility, followed by the path to the submission script:</p> <pre><code>sbatch myScript.sh\n</code></pre> <p>You should get a message in return with a JobID. This number is unique to every job on the system and can be used to manage or monitor your job.</p>"},{"location":"guides/slurm/#job-management","title":"Job Management","text":""},{"location":"guides/slurm/#modify-job","title":"Modify Job","text":"<p>After scheduling your job you still have limited options to modify your job.  If your job has not started yet, you are able to change almost all parameters. If it is running already, a lot of things will be locked the way it is.</p> <p>To modify the job use the <code>scontrol</code> utility. If you run <code>scontrol show JobID=&lt;JobID&gt;</code> you see all available parameters. Here an example where we lower the runtime limit to 3 days:</p> <pre><code>scontrol update JobID=&lt;JobID&gt; TimeLimit=3-00:00:00\n</code></pre>"},{"location":"guides/slurm/#delete-cancel-job","title":"Delete / Cancel Job","text":"<p>To delete a job you can use the <code>scancel</code> utility followed by your JobID.</p> <pre><code>scancel &lt;JobID&gt;\n</code></pre>"},{"location":"guides/slurm/#job-monitoring","title":"Job Monitoring","text":"<p>You have different ways to keep track of your jobs, wether they are done or still running</p>"},{"location":"guides/slurm/#actively-running-jobs","title":"Actively Running jobs","text":"<p>To get a list of all running jobs on the platform you can use the <code>squeue</code> utility. You can filter by user using the <code>-u</code> parameter or by JobID using <code>-j</code>:</p> <pre><code>squeue -u &lt;guid&gt; -j &lt;JobID&gt;\n</code></pre>"},{"location":"guides/slurm/#historic-job-analysis","title":"Historic Job Analysis","text":"<p>Slurm keeps a database with information of all jobs run using the system. To access this data, you can use the <code>sacct</code> command.  Use the <code>-o</code> parameter followed by a list of Job-Accounting-Fields. A list of all available Job-Accounting-Fields can be found here.</p> <p>Per default you will get information for each job step in your output. If you don't want more than one line for each of your jobs, you can use the <code>-X</code> parameter.</p> <p>You can  get information for a specific job  using the <code>-j</code> parameter, followed by your JobID:</p> <pre><code>sacct -X -j &lt;JobID&gt; -o JobID,JobName,User,Submit,Start,End\n</code></pre> <p>Or more broadly all the jobs run by your user, using the <code>-u</code> parameter with your GUID:</p> <pre><code>sacct -X -u &lt;guid&gt; -o JobID,JobName,User,Submit,Start,End\n</code></pre> <p>You can further restrict your output with different parameters. One of the more common ones are the <code>--starttime</code> and <code>--endtime</code> with these you can  define in what period you want to lookup jobs for. If you only specify <code>--starttime</code>, the endtime will  be set to now. Here an example, where you see all your jobs in the last 7 days:</p> <pre><code>sacct -X -u &lt;guid&gt; --starttime $(date -d \"-7days\" +%Y-%m-%d) -o JobID,JobName,User,Submit,Start,End\n</code></pre>"},{"location":"guides/slurm/#job-efficiency","title":"Job Efficiency","text":"<p>For CPU and Memory efficiency, you can easily use the <code>seff</code> command. If you provide it with the JobID you want to analyse, it will show you how efficient the requested resources were used. The numbers can only be fully trusted after the job has finished running.</p> <pre><code>$ seff &lt;JobID&gt;\nJob ID: &lt;JobID&gt;\nCluster: &lt;cluster-name&gt;&gt;\nUser/Group: &lt;giod&gt;/&lt;group&gt;\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:01:00\nCPU Efficiency: 49.18% of 00:02:02 core-walltime\nJob Wall-clock time: 00:01:01\nMemory Utilized: 5.97 GB\nMemory Efficiency: 18.64% of 32.00 GB\n</code></pre> <p>You can see only 50% of the requested CPU resources were used, this means in this case you could go down from 2 CPUs to 1. For CPU efficiency, we want to get as close to 100% as possible.</p> <p>The memory usage was also overestimated and could go down a bit too. For memory, it is fine to keep a bit of a buffer, as you don\u2019t want to risk getting an Out Of Memory (OOM) error. I would aim to stay above 70% efficiency.</p> <p>To see if your time estimation was right, you can compare the requested with the actual time with the <code>sacct</code> command and adjust accordingly. It is fine to have a a time buffer, in case there are minor delays in the command. In this example below, you could adjust the time limit to 2 hours.</p> <pre><code>$ sacct -X -o Timelimit,Elapsed -j &lt;JobID&gt;\n Timelimit    Elapsed\n---------- ----------\n  04:00:00   01:48:33\n</code></pre> <p>There is no easy way to get GPU efficiency, but generally speaking if you don\u2019t need GPU, don\u2019t request it.</p>"},{"location":"policies/website-privacy-policy/","title":"Website Privacy Policy","text":"<p>The University of Glasgow will be what\u2019s known as the \u2018Data Controller\u2019 of your personal data processed in relation to your use of this website. This privacy notice will explain how The University of Glasgow will process your personal data. This notice does not cover the links within this site to other non-University sites nor the content of external internet sites.</p>"},{"location":"policies/website-privacy-policy/#who-we-are","title":"Who we are","text":"<p>Research Computing as a Service (RCaas) is a function within Information Services of the University of Glasgow. The website is operated and hosted by RCaas.</p>"},{"location":"policies/website-privacy-policy/#why-we-need-to-collect-your-personal-data","title":"Why we need to collect your personal data","text":"<p>We are collecting your personal data in order to provide this website to you. We will only collect data that is necessary for this purpose.</p>"},{"location":"policies/website-privacy-policy/#what-personal-data-do-we-collect","title":"What personal data do we collect","text":""},{"location":"policies/website-privacy-policy/#log-files","title":"Log Files","text":"<p>The website does not automatically capture or store personal data from visitors to the website, other than to log your IP address and session information. Session information includes the time and duration of your visit to the site, the files requested, and the browser used. This information will only be accessed by authorised persons of the University or its agents. The information will be retained by the University and will only be used for the purpose of (a) managing the site system, (b) the identification of broken links, and (c) for statistical and audit purposes.</p>"},{"location":"policies/website-privacy-policy/#embedded-content-from-other-websites","title":"Embedded content from other websites","text":"<p>Articles on this site may include embedded content (e.g. videos, images, articles, etc.). Embedded content from other websites behaves in the exact same way as if the visitor has visited the other website.</p> <p>These websites may collect data about you, use cookies, embed additional third-party tracking, and monitor your interaction with that embedded content, including tracking your interaction with the embedded content if you have an account and are logged in to that website.</p>"},{"location":"policies/website-privacy-policy/#how-long-we-retain-your-data","title":"How long we retain your data","text":"<p>Your data will be retained by the University for as long as is necessary to fulfil the purpose. After this time, data will be securely deleted.</p>"},{"location":"policies/website-privacy-policy/#what-rights-you-have-over-your-data","title":"What rights you have over your data","text":"<p>You have the right to:</p> <ul> <li>be informed as to how we use your data (via this privacy notice)</li> <li>access or request a copy of the data we hold about you (see below)</li> <li>opt out of specific types of processing</li> <li>ask us to remove your data from our records (see below)</li> <li>withdraw consent, where it is used as a legal basis for processing.</li> </ul> <p>You can request access to the information we process about you at any time. If at any point you believe that the information we process relating to you is incorrect, you can request to see this information and may in some instances request to have it restricted, corrected or, erased. You may also have the right to object to the processing of data and the right to data portability.</p> <p>If you wish to exercise any of these rights, please submit your request via the webform or contact dp@gla.ac.uk.</p> <p>*Please note that the ability to exercise these rights will vary and depend on the legal basis on which the processing is being carried out.</p>"},{"location":"policies/website-privacy-policy/#complaints","title":"Complaints","text":"<p>If you wish to raise a complaint on how we have handled your personal data, you can contact the University Data Protection Officer who will investigate the matter.</p> <p>Our Data Protection Officer can be contacted at dataprotectionofficer@glasgow.ac.uk If you are not satisfied with our response or believe we are not processing your personal data in accordance with the law, you can complain to the Information Commissioner\u2019s Office (ICO) https://ico.org.uk/</p>"},{"location":"policies/lochan/cluster-usage-policy/","title":"Lochan: Cluster Usage Policy","text":"<p>To ensure the stability and usability of the platform for all users, there are certain rules and policies to abide by when using Lochan. This is a living document, so changes can be made at any time. If you have any questions regarding the contents of this page, feel free to get in contact with the admins via Ivanti.</p> <ol> <li>Jobs must be run using the Slurm queueing system, interactive or batch jobs. Logging into any worker-nodes directly without a running allocation is forbidden.</li> <li>No compute intensive tasks are to be run on the login node. The login-node is to be used for minor text editing, data management, submitting and monitoring jobs. The admin team reserves the right to kill processes, that constrain the login node, without warning.</li> <li>Job submissions must specify the resources to the best estimate of the user, as to not waste cluster resources. This ensures the queueing system runs optimally and therefore the user\u2019s jobs are starting as soon as possible. The admin team reserves the right delete poorly specified jobs.</li> <li>Any data left on scratch spaces after the runtime of the job, may be deleted at any time and without notice.</li> <li>All cluster storage is to be used only for active processing, and not for long term storage purposes. Please ensure, that all your scripts, datasets and results are saved to another system, as the admin team offers limited data recovery services.</li> <li>The admin team reserves the right to delete, kill or hold jobs or processes that have an adverse effect on the whole system\u2019s stability or health.</li> <li>The job submission system has the capability to send automated emails about the progress of jobs. Users should ensure that the number of requested emails is not excessive and refrain from sending emails to any addresses outside the University of Glasgow domain. This means that affiliate/external user accounts should be using a University of Glasgow email address for job alerting.</li> <li>The cluster is to be used for research purposes. Teaching and courses are not to be held using this cluster unless it was agreed upon prior with the admin team.</li> <li>To ensure the stability and security of the cluster, maintenance is needed. This might result in access being restricted to the cluster or parts of it for certain periods of time. These maintenance windows will be announced via our Team on Microsoft Teams.</li> </ol> <p>Please note that in addition to the Cluster Usage Policy users must also comply with the general IT Policy of the University of Glasgow.</p>"},{"location":"policies/lochan/project-terms-condition/","title":"Lochan: Project Policy","text":"<p>If you are a PI of a Project you have to agree and abide by policy below.</p> <ol> <li>The storage space provided for a Project should only home data that is used for HPC and not be used for long term or archival purposes.</li> <li>The storage space allocated for a Project is not to be used as primary storage. The PI and their members are responsible to set up their own backups.</li> <li>The PI of a Project is responsible to confirm or deny any requested access by users. On request the admin team can provide a list of all users with access.</li> <li>If the space is no longer required, the PI is responsible for approaching the admin team about retiring the Project.</li> <li>The admin team will approach the PI of a Project on a yearly basis, to verify the continued usage of the Project. Where this is not confirmed or proven, the Project might be retired.</li> <li>Upon retirement of a Project, the PI and their members are responsible for copying all data still required from the storage space allocated, within one month. After an additional three months, all data is permanently deleted.</li> </ol>"},{"location":"references/pbs-2-slurm/","title":"PBS to Slurm Cheat Sheet","text":""},{"location":"references/pbs-2-slurm/#user-commands","title":"User Commands","text":"Purpose PBS Slurm Submit a job via script <code>qsub myScript.sh</code> <code>sbatch myScript.sh</code> Cancel a job <code>qdel &lt;JobID&gt;</code> <code>scancel &lt;JobID&gt;</code> List all running jobs <code>qstat</code> <code>squeue</code> List all your running jobs <code>qstat -u &lt;GUID&gt;</code> <code>squeue -u &lt;GUID&gt;</code> Advanced job information <code>qstat -f &lt;JobID&gt;</code> <code>sacct -j &lt;JobID&gt;</code>"},{"location":"references/pbs-2-slurm/#environment-variables","title":"Environment Variables","text":"Purpose PBS Slurm Job ID <code>$PBS_JOBID</code> <code>$SLURM_JOBID</code> Submit directory <code>$PBS_O_WORKDIR</code> <code>$SLURM_SUBMIT_DIR</code> Allocated node list <code>$PBS_NODEFILE</code> <code>$SLURM_JOB_NODELIST</code> Current node name <code>-</code> <code>$SLURMD_NODENAME</code> Array job index <code>$PBS_ARRAY_INDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> Number of CPUs <code>$PBS_NUM_PPN * $PBS_NUM_NODES</code> <code>$SLURM_NPROCS</code>"},{"location":"references/pbs-2-slurm/#job-parameters","title":"Job Parameters","text":"Purpose PBS Slurm Job name <code>#PBS -N myJob</code> <code>#SBATCH --job-name=myJob</code> Wallclock limit <code>#PBS -l walltime=hh:mm:ss</code> <code>#SBATCH --time=dd-hh:mm:ss</code> CPU Time <code>#PBS -l cput=hh:mm:ss</code> <code>-</code> Number of nodes and CPUs <code>#PBS -l nodes=2:ppn=8</code> <code>#SBATCH --nodes=2 --ntasks-per-node=1 --cpus-per-task=8</code> Memory <code>-</code> <code>#SBATCH --mem=8G</code> GPU <code>-</code> <code>#SBATCH --gres=gpu:1</code> Array <code>#PBS -t 1-100</code> <code>#SBATCH --array=1-100</code> Select queue / partition <code>#PBS -q &lt;QueueName&gt;</code> <code>#SBATCH --partition=&lt;PartitionName&gt;</code> Working directory <code>-</code> <code>#SBATCH --workdir=/path/to/file</code> STDOUT file <code>#PBS -o /path/to/file</code> <code>#SBATCH --output=/path/to/file</code> STDERR file <code>#PBS -e /path/to/file</code> <code>#SBATCH --error=/path/to/file</code> Email notifications <code>#PBS -m a\\|b\\|e</code> <code>#SBATCH --mail-type=NONE\\|BEGIN\\|END\\|FAIL\\|ALL</code> Email recipient <code>#PBS -M &lt;Email&gt;</code> <code>#SBATCH --mail-user=&lt;Email&gt;</code>"},{"location":"references/slurm-parameters/","title":"Slurm Parameters","text":"<p>You can use a whole lot of parameters to specify your resource request to the scheduler. We will cover the most common parameters here, but if you want a full overview of all available ones and extra information, you can run <code>man sbatch</code> or <code>man srun</code> when logged into the system. </p> Parameter Example Description <code>-J, --job-name=&lt;jobname&gt;</code> <code>--job-name=\"Test-Job-1\"</code> Specify a name for the job. The specified name will appear along with the job id number when querying running jobs on the system. The default is the supplied executable program's name. <code>-p, --partition=&lt;partition_names&gt;</code> <code>--partition=gpu</code> Request a specific partition for the resource allocation. If the job can use more than one partition, specify their names in a comma separate list. If the parameter is not set it defaults to configured \"default partition\". <code>-o, --output=&lt;path&gt;</code> <code>--output=/path/to/file/%x-%j.out</code> Write all output of <code>STDOUT</code> to file. You can use dynamic parameters for filenames, like <code>%x</code> for job name and <code>%j</code> for jobID. <code>-e, --error=&lt;path&gt;</code> <code>--error=/path/to/file/%x-%j.err</code> Write all output of <code>STDERR</code> to file. You can use dynamic parameters for filenames, like <code>%x</code> for job name and <code>%j</code> for jobID. See \"filename pattern\" in manual for more information. <code>-D, --chdir=&lt;directory&gt;</code> <code>--chdir=/path/to/workdir</code> Set the working directory of the batch script to the specified relative or full path before it is executed <code>-t, --time=&lt;dd-hh-mm-ss&gt;</code> <code>--time 00-01:00:00</code> Set a limit on the total run time of the job allocation. When the time limit is reached, each task in each job step is sent a kill signal. <code>-b, --begin=&lt;time&gt;</code> <code>--begin=2025-01-01T12:00:00</code> If you want a job to run at a specific time, you can set a start time. The start time is not guaranteed. <code>-d, --dependency=&lt;condition&gt;:&lt;jobID&gt;</code> <code>--dependency=afterok:9999</code> Set a dependency for your job to run. In the example, the job will run if job 9999 finished successfully. Read the manual for more configuration opetions. <code>-N, --nodes=&lt;num&gt;</code> <code>--nodes=2</code> Number of servers you want your job to run on. Use only if your code supports parallel processing. <code>-n, --ntasks=&lt;num&gt;</code> <code>--ntasks=4</code> Number of Slurm tasks to be launched, increase for multi-process runs ex. MPI. <code>--ntasks-per-node=&lt;num&gt;</code> <code>--ntasks-per-node=4</code> Number of Slurm tasks to be launched per node. Helpful to evenly distribute workload between nodes (servers). <code>-c, --cpus-per-task=&lt;num&gt;</code> <code>--cpus-per-task=8</code> Request the number of CPUs to be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. <code>--gres=gpu:&lt;type&gt;:&lt;num&gt;</code> <code>--gres=gpu:gtx1080:1</code> Specify the type and number of GPUs for your allocation. <code>--mem=&lt;num&gt;&lt;unit&gt;</code> <code>--mem=16G</code> Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K <code>--mail-user=&lt;email&gt;</code> <code>--mail-user=\"example@example.co.uk\"</code> Email address to send notifications to. Only University of Glasgow managed emails are accepted. <code>--mail-type=&lt;type&gt;</code> <code>--mail-type=\"BEGIN,END,FAIL\"</code> Comma separated list of event types <code>--mail-user</code> gets notified for.  Valid type values are NONE, BEGIN, END, FAIL, RE\u2010QUEUE, ALL"},{"location":"references/submission-script-templates/","title":"Submission Script Templates","text":"<p>The scripts below can be run on the platform as is. You can expand them with parameters from Slurm Parameters.</p>"},{"location":"references/submission-script-templates/#cpu-job","title":"CPU Job","text":"Lochan <pre><code>#!/bin/bash -l\n\n############# SLURM SETTINGS #############\n#SBATCH --job-name=myjob        # some descriptive job name of your choice\n#SBATCH --partition=cpu         # which partition to use, default \"cpu\"\n#SBATCH --time=0-01:00:00       # time limit for the whole run, in the form of d-hh:mm:ss\n#SBATCH --mem=1G                # memory required per node, in the form of [num][M|G|T]\n#SBATCH --nodes=1               # number of nodes to allocate, default is 1\n#SBATCH --ntasks=1              # number of Slurm tasks to be launched, increase for multi-process runs ex. MPI\n#SBATCH --cpus-per-task=1       # number of processor cores to be assigned for each task, default is 1, increase for multi-threaded runs\n#SBATCH --ntasks-per-node=1     # number of tasks to be launched on each allocated node\n\n############# SOFTWARE SETUP #############\n#module load xxx\n#export PATH=\"$PATH:/mnt/home/guid/software/xxx/bin\"\n\n############# MY CODE #############\necho \"Hello from $SLURM_JOB_NODELIST\"\n</code></pre>"},{"location":"references/submission-script-templates/#gpu-job","title":"GPU Job","text":"Lochan <pre><code>#!/bin/bash -l\n\n############# SLURM SETTINGS #############\n#SBATCH --job-name=myjob        # some descriptive job name of your choice\n#SBATCH --partition=gpu         # which partition to use, default \"cpu\"\n#SBATCH --time=0-01:00:00       # time limit for the whole run, in the form of d-hh:mm:ss\n#SBATCH --mem=1G                # memory required per node, in the form of [num][M|G|T]\n#SBATCH --nodes=1               # number of nodes to allocate, default is 1\n#SBATCH --ntasks=1              # number of Slurm tasks to be launched, increase for multi-process runs ex. MPI\n#SBATCH --cpus-per-task=1       # number of processor cores to be assigned for each task, default is 1, increase for multi-threaded runs\n#SBATCH --ntasks-per-node=1     # number of tasks to be launched on each allocated node\n#SBATCH --gres=gpu:gtx1080:1    # type and number of GPU, in the format gpu:[type]:[num]\n\n############# SOFTWARE SETUP #############\n#module load xxx\n#export PATH=\"$PATH:/mnt/home/guid/software/xxx/bin\"\n\n############# MY CODE #############\necho \"Hello from $SLURM_JOB_NODELIST\"\nnvidia-smi\n</code></pre>"},{"location":"references/submission-script-templates/#array-job","title":"Array Job","text":"Lochan <pre><code>#!/bin/bash -l\n\n############# SLURM SETTINGS #############\n#SBATCH --job-name=myjob        # some descriptive job name of your choice\n#SBATCH --partition=cpu         # which partition to use, default \"cpu\"\n#SBATCH --time=0-01:00:00       # time limit for the whole run, in the form of d-hh:mm:ss\n#SBATCH --mem=1G                # memory required per node, in the form of [num][M|G|T]\n#SBATCH --nodes=1               # number of nodes to allocate, default is 1\n#SBATCH --ntasks=1              # number of Slurm tasks to be launched, increase for multi-process runs ex. MPI\n#SBATCH --cpus-per-task=1       # number of processor cores to be assigned for each task, default is 1, increase for multi-threaded runs\n#SBATCH --ntasks-per-node=1     # number of tasks to be launched on each allocated node\n\n############# SOFTWARE SETUP #############\n#module load xxx\n#export PATH=\"$PATH:/mnt/home/guid/software/xxx/bin\"\n\n############# MY CODE #############\necho \"Hello from $SLURM_JOB_NODELIST\"\necho \"This is task $SLURM_ARRAY_TASK_ID out of $SLURM_ARRAY_TASK_COUNT\"\nexho \"Jobs are between $SLURM_ARRAY_TASK_MIN and $SLURM_ARRAY_TASK_MAX\"\n</code></pre>"},{"location":"tutorials/build-from-source/","title":"Building Software from Source","text":"<p>Open source software developers, often give you the option to build their software from the source code. This allows you to install a software yourself with all the customisations fit for your work.</p> <p>These installations are very individual to each software, so pay close attention to the developers documentation and installation manual. The page here is to explain some core concepts and to build your first installation from source.</p> <p>For our example we will be building <code>python3</code> from source. The instructions are from the README.md file of the cpython repository.</p> <p>Let\u2019s create a directory where we want the installation to sit. This is going to be the directory where all your binaries and application files for python3 will reside:</p> <pre><code>mkdir --parents ~/mysoftware/python3/3.13.2\n</code></pre> <p>For larger installations we recommend doing them within an interactive batch job if possible!</p>"},{"location":"tutorials/build-from-source/#dependencies","title":"Dependencies","text":"<p>Sometimes software requires preinstalled dependencies to be built. These packages are often called <code>*-devel</code> and contain the header files for common libraries used during the build process. But it can also be other dependant software like <code>openMPI</code> or <code>nvidia-cuda</code>. In some cases you might be able to just load the appropriate modules on the cluster.</p> <p>As you don\u2019t have <code>sudo</code> / <code>root</code> access on the cluster, you won\u2019t be able to install any dependencies through the package manager (<code>dnf</code>, <code>yum</code>, <code>apt</code>), as some installation manuals make you believe. To get around this you have some options:</p> <ul> <li>You can also build all the dependencies yourself from source.</li> <li>You can make use of conda environments or containers.</li> <li>You can contact the RCaaS HPC admin team for help, who can look to install missing packages.</li> </ul> <p>For our <code>python3</code> example, no dependencies have to be installed.</p>"},{"location":"tutorials/build-from-source/#download-source-code","title":"Download source code","text":"<p>Source code is often made available through a tarball. This is a usually compressed archive containing all the source code. To download this, you can use the <code>wget</code> utility followed by the link to the tarball file. In our case this is:</p> <pre><code>wget https://www.python.org/ftp/python/3.13.2/Python-3.13.2.tgz\n</code></pre> <p>To then unpack this archive, you can use the <code>tar</code> utility:</p> <pre><code>tar --extract --file Python-3.13.2.tgz\n</code></pre> <p>Please be aware, that archives might be <code>.zip</code> and therefore require the <code>unzip</code> utility to unpack. Some software might also expect you to download their source from a git repository. For this you can use the <code>git</code> utility.</p>"},{"location":"tutorials/build-from-source/#compile-the-code","title":"Compile the code","text":"<p>Python3 is using the GNU Autotools as its build tool, which is very popular. Another one you might come across quite often is Cmake. Just follow the developers installation manual.</p> <p>First we move into the directory we unpacked above:</p> <pre><code>cd Python-3.13.2\n</code></pre> <p>Now, we will prepare the <code>Makefile</code>, using the <code>.configure</code> script provided by the developers. This step will ensure dependencies are satisfied and sets desired build options. This is where you can customise your installation!</p> <p>To get a list of all available build options you can run:</p> <pre><code>./configure --help\n</code></pre> <p>You will see there is a big list! You can read up on what all of these options mean and decide for yourself what you want to change. For our example, there is only one we will be using, <code>--prefix</code>. This parameter decides where our installation will reside. As a default it will try to install the software in the system directories, which only <code>root</code> has access to. This is the parameter you will most likely need to adjust for all your installations, to ensure you can do installations without requiring elevated privileges.</p> <p>For our prefix, we choose the directory we created at the beginning:</p> <pre><code>./configure --prefix=$HOME/mysoftware/python3/3.13.2\n</code></pre> <p>Now we can run <code>make</code>, to compile the software based on the information gathered in the <code>Makefile</code>. This step can take a long time depending how complicated the software you are building is:</p> <pre><code>make\n</code></pre> <p>As a last step, this will copy the binaries and other application files into the directory we set as our <code>--prefix</code>, from where we can then make use of them:</p> <pre><code>make install\n</code></pre> <p>To test if our installation was successful, lets run the <code>python3</code> binary we built!</p> <pre><code>$ $HOME/mysoftware/python3/3.13.2/bin/python3 --version\nPython 3.13.2\n</code></pre> <p>If we are confident our installation was successful, we can delete all our build files and source code, to save storage:</p> <pre><code>rm -rf Python-3.13.2\n</code></pre>"},{"location":"tutorials/build-from-source/#adjust-environment","title":"Adjust Environment","text":"<p>To make the application files seamlessly available in your environment, you have to change some environment variables. The environment variables are set as such:</p> <pre><code>export &lt;var-name&gt;=\"&lt;path1&gt;:&lt;path2&gt;:&lt;path3&gt;\"\n</code></pre> <p>For all these variables, it works on a first match basis. The system goes through all paths listed and the first match it finds will be the executable run. So to ensure, our own installation is used before the system installation, our newly added paths have to be before the system paths. Ensure, to always add the current <code>$&lt;var-name&gt;</code> when updating your variables, to ensure you don\u2019t lose access to any software!</p> <p>For the executables, we will add the created bin directory to the <code>$PATH</code> variable:</p> <pre><code>export PATH=\"$HOME/mysoftware/python3/3.13.2/bin:$PATH\"\n</code></pre> <p>For our libraries, we add the lib path to the <code>$LD_LIBRARY_PATH</code> variable:</p> <pre><code>export LD_LIBRARY_PATH=\"$HOME/mysoftware/python3/3.13.2/lib:$LD_LIBRARY_PATH\"\n</code></pre> <p>And to make sure we have the right manual pages for our version we add the path to the <code>man</code> pages to <code>$MANPATH</code>:</p> <pre><code>export MANPATH=\"$HOME/mysoftware/python3/3.13.2/share/man:$MANPATH\"\n</code></pre> <p>Now your installation should be seamlessly included in your software stack.</p> <pre><code>$ python3 --version\nPython 3.13.2\n</code></pre> <p>If you don\u2019t want this to get lost when you next log in, you can add it to your <code>~/.bashrc</code> file.</p> <pre><code>$ cat ~/.bashrc\n...\nexport PATH=\"$HOME/mysoftware/python3/3.13.2/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"$HOME/mysoftware/python3/3.13.2/lib:$LD_LIBRARY_PATH\"\nexport MANPATH=\"$HOME/mysoftware/python3/3.13.2/share/man:$MANPATH\"\n...\n</code></pre>"},{"location":"tutorials/slurm-101/","title":"Slurm for Beginners","text":"<p>This small tutorial should give you a start to the world of Slurm! If you have any questions or issues, feel free to get in contact with the admins via Ivanti! We help novice and expert users alike to get settled.</p> <p>If you are not familiar with the UNIX console, please do the UNIX Tutorial for Beginners first.</p> <p>For this whole tutorial you will find code snippets, these are displayed as interactions on a console. Lines with a preceding $ are the commands entered, ones without, are the output of these commands.</p> <pre><code>$ &lt;command-entered-by-user&gt;\n&lt;command-output&gt;\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-1-scheduler","title":"Lesson 1: Scheduler","text":"<p>The Scheduler, is the program running on the cluster, that schedules your jobs to be run on the resources available within the cluster. It ensures fair usage, and should under no circumstances be bypassed. The HPC managed by the University of Glasgow use Slurm as their scheduler.</p> <p>Look at what resources you have available to you on the cluster, using the Slurm <code>sinfo</code> command. (The output displayed below is shortened)</p> <pre><code>$ sinfo -o \"%n %c %m %G\" | column -t\nHOSTNAMES  CPUS  MEMORY   GRES\nnode021     64    514048   (null)\nnode022     64    515072   (null)\nnode023     64    515072   (null)\n...\n</code></pre> <p>The nodes are grouped into partitions, this is a way to define what type of machine you want to work with. They are sometimes also referred to as queues. You can list all available partitions also using the <code>sinfo</code> command:</p> <pre><code>$ sinfo -o \"%P %D %N\" | column -t\nPARTITION  NODES  NODELIST\nnodes*     10     node[01-10]\ngpu        3      gpu[01-20]\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-2-interactive-job","title":"Lesson 2: Interactive Job","text":"<p>Interactive jobs are great to install software, prepare your environment or debug your script. It is also a good point to start to engage with HPC. But please, for any serious or larger work, use batch jobs. More on those in Lesson 3!</p> <p>You can start an interactive job using the <code>srun</code> command. You can tell, you are on a different server by the prompt, which should now feature the name of a compute node:</p> <pre><code>[login1]$ srun --pty bash\n[node01]$\n</code></pre> <p>Similar from how you switched from your PC via <code>ssh</code> to the login node, we now switched from the login node to the compute node using Slurm.</p> <p>In this session, we can start doing computational work. As an example, we will run some python3 code.</p> <ol> <li> <p>Run python3 <pre><code>$ python3\n</code></pre></p> </li> <li> <p>Import python libraries <pre><code>&gt;&gt;&gt; import os\n</code></pre></p> </li> <li> <p>Get Slurm node name <pre><code>&gt;&gt;&gt; node = os.getenv(\"SLURMD_NODENAME\")\n</code></pre></p> </li> <li> <p>Open a new text file <pre><code>&gt;&gt;&gt; myfile = open(\"demofile.txt\", \"w\")\n</code></pre></p> </li> <li> <p>Write into the text file <pre><code>&gt;&gt;&gt; myfile.write(\"I am doing compute work on \" + node)\n</code></pre></p> </li> <li> <p>Exit python3 <pre><code>&gt;&gt;&gt; exit()\n</code></pre></p> </li> </ol> <p>Look at the file you just created using <code>cat</code>. You can see it used the name of the node we are working on in the output.</p> <pre><code>$ cat demofile.txt\nI am doing compute work on node01\n</code></pre> <p>This is our interactive job done, lets close our session with the exit command. This should bring us back to the login node, as indicated by the prompt again:</p> <pre><code>[node01]$ exit\nexit\n[login1]$\n</code></pre>"},{"location":"tutorials/slurm-101/#lesson-3-batch-job","title":"Lesson 3: Batch Job","text":"<p>Interactive jobs are not very easily reproducible and generally take longer, as you have to type or paste every command yourself into the console. It is also not guaranteed, that your jobs will run right away, meaning you could be waiting hours for your interactive job to start! This is why we encourage users to use batch jobs. With batch job submission, you can schedule your work, log off the system and come back once your work is finished, fully autonomous.</p> <p>First, we have to save our python code into a file. For this create a file with the <code>.py</code> ending and add the lines we executed in python above in the interactive job. To make sure the output of our script is unique we will add the JobID as an identifier to the output file of the script.</p> <p>You can create and edit files using either <code>vi</code> or <code>nano</code>, whatever is more comfortable to you. Nano is the easier of the two, so if you are unfamiliar, we recommend using it. To exit and save in Nano do <code>ctrl+X \u2192 Y \u2192 Enter</code>.</p> <p><pre><code>$ nano myPythonCode.py\n</code></pre> <pre><code>import os\nnode = os.getenv(\"SLURMD_NODENAME\")\njobid = os.getenv(\"SLURM_JOBID\")\nmyfile = open(\"demofile-\" + jobid + \".txt\", \"w\")\nmyfile.write(\"I am doing compute work on \" + node)\nexit()\n</code></pre></p> <p>Now to schedule our job with Slurm , we need to create a job script. You can use a template from Submission Script Templates</p> <p>In the \u201cMy Code\u201d section run the script you saved earlier by providing it as a parameter to the python3 executable:</p> <pre><code>\u2026\n############# MY CODE #############\npython3 myPythonCode.py\n</code></pre> <p>Now you can submit your job using the <code>sbatch</code> utility and the path to the script. You will get a JobID in return, remember this to find your output later:</p> <pre><code>$ sbatch myPythonJob.sh\nSubmitted batch job &lt;JobID&gt;\n</code></pre> <p>After the job has finished, within your current working directory you should find an output file, which contains the <code>STDOUT</code> and an error file, which contains the <code>STDERR</code> of your job session. In addition, the file your python script created should also be there. Since your home storage is shared across all servers, you can see the output of your scripts in real time from the login node, even if the job ran on a compute node:</p> <pre><code>$ ls -l\nmyjob-&lt;JobID&gt;.out\nmyjob-&lt;JobID&gt;.err\ndemofile-&lt;JobID&gt;.txt\n</code></pre> <p>Check the output of these files. If all went as planned, the error file should be empty.</p> <p>You can run this job as many times as you want by just using the <code>sbatch</code> command. Like this you have access to the power of the HPC compute nodes, without ever having to log into one yourself.</p>"},{"location":"tutorials/slurm-101/#lesson-4-monitoring","title":"Lesson 4: Monitoring","text":"<p>You can see all running jobs on the system using the <code>squeue</code> command. If you only want to see your own jobs, you can specify your username with the <code>-u</code> parameter. If you only want to see a specific job, you can use the <code>-j</code> parameter followed by your JobID.</p> <pre><code>$ squeue\n$ squeue -u &lt;GUID&gt;\n$ squeue -j &lt;JobID&gt;\n</code></pre> <p>The output is as follows:</p> <ul> <li>JOBID: Unique identifier of the job. Counts up from 1 being the first job ever queued.</li> <li>PARTITION: Scheduler partition the job is queued into.</li> <li>NAME: Name of the job as defined by <code>--job-name</code> in your submission or your script name.</li> <li>USER: User who submitted the job to the scheduler.</li> <li>ST: Status of the job: <code>R</code>=Running, <code>PD</code>=Pending (Queued).</li> <li>TIME: Time the job has been running for.</li> <li>NODES: Number of nodes.</li> <li>NODELIST: List of names of allocated nodes.</li> </ul> <p>Slurm keeps a database with information of all jobs run using the system. To access this data, you can use the <code>sacct</code> command. Using the JobID you saved from your job, we can show a wide list of information for your job. Use the <code>-o</code> parameter followed by a list of Job Accounting Fields.</p> <p>A list of all available Job Accounting Fields can be found here</p> <p>Below is an example that gives you an overview of the requested resources for a job. You should see, these are the values you provided in the \u201cSLURM SETTINGS\u201d section of your submission script:</p> <pre><code>$ sacct -X -j &lt;JobID&gt; -o JobID,User,ReqCPUS,ReqMem,ReqNodes,TimeLimit\n</code></pre> <p>Now let\u2019s get some more information on how and where our job ran. In this output we see how long the job ran, and how it completed. An <code>ExitCode</code> of anything other than <code>0</code> usually means there was an error:</p> <pre><code>$ sacct -X -j &lt;JobID&gt; -o JobID,NodeList,Start,End,Elapsed,State,ExitCode\n</code></pre> <p>Say we want to check if our job ran efficiently, we could use the <code>seff</code> command. It uses data from the Slurm accounting database, to create information on how efficiently your job ran. We can use this information to make our jobs more efficient:</p> <pre><code>$ seff &lt;JobID&gt;\n</code></pre> <p>These efficiency values are only accurate, once your job has finished running. Accurate job scripts help the queuing system efficiently allocate shared resources. And therefore, your jobs should run quicker.</p>"},{"location":"tutorials/unix-101/","title":"UNIX for Beginners","text":"<p>This small tutorial should give you a start to the world of UNIX console! If you have any questions or issues, feel free to contact us! We help novice and expert users alike to get settled.</p> <p>For this whole tutorial you will find code snippets, these are displayed as interactions on a console. Lines with a preceding $ are the commands entered, ones without, are the output of these commands.</p> <pre><code>$ &lt;command-entered-by-user&gt;\n&lt;command-output&gt;\n</code></pre> <p>For all commands mentioned in the tutorial and most other UNIX commands, you can find a manual page with all the information about it. To access it just run <code>man</code> followed by the command. Here an example with <code>ls</code>:</p> <pre><code>$ man ls\n</code></pre> <p>Use the arrow keys to move up and down on the manual and press <code>q</code> on your keyboard to exit.</p>"},{"location":"tutorials/unix-101/#lesson-1-filesystem","title":"Lesson 1: Filesystem","text":"<p>The Filesystem is build hierarchically. When first logging into the system with SSH, you\u2019ll get put into your home directory. In our example, this is <code>/users/&lt;GUID&gt;</code>, where <code>users</code> is the parent directory of your home. The prepending <code>/</code> (root) signifies that it is a full path rather than a relative path, which starts from where you are currently at in the hierarchy.</p> <p>As an example for this lesson, let\u2019s take this directory structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Documents\n      \u2514 myFile.txt\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home <code>/users/xx999x</code>.</p> <p>The full path to the file <code>myFile.txt</code> is: <code>/users/xx999x/Documents/myFile.txt</code></p> <p>The relative path from your position after logging in is: <code>Documents/myFile.txt</code></p>"},{"location":"tutorials/unix-101/#listing-files-and-directories","title":"Listing Files and Directories","text":"<p>To list the contents of a directory use the <code>ls</code> command, where ls stands for list:. Using it without arguments will show you the contents of the directory you are currently in:</p> <pre><code>$ ls\nDocuments Images\n</code></pre> <p>Files with a prepending <code>.</code> are hidden files and will not be listed by default. To also show hidden files use the <code>-a</code> parameter:</p> <pre><code>$ ls -a\n. .. .bashrc Documents Images\n</code></pre> <p>You can supply a path to a directory you wish to list the contents of after the <code>ls</code> command:</p> <pre><code>$ ls Documents\nmyFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#changing-directories","title":"Changing Directories","text":"<p>To show the full path of the directory you are currently in you can use the <code>pwd</code> command, where pwd stands for print working directory:</p> <pre><code>$ pwd\n/users/xx999x\n</code></pre> <p>If you want to move around in the file system you can use the <code>cd</code> command, where cd stands for change directory. Using <code>cd</code> without arguments puts you back into your home directory.</p> <pre><code>$ pwd\n/users/xx999x/Documents\n$ cd\n$ pwd\n/users/xx999x\n</code></pre> <p>If you want to move into a specific directory you have to give the path to that directory as an argument:</p> <pre><code>$ pwd\n/users/xx999x\n$ cd Documents\n$ pwd\n/users/xx999x/Documents\n</code></pre> <p>Within every directory there are two directories, <code>.</code> for the current working directory and <code>..</code> for the parent directory. So if you want to go back, you can change into this directory:</p> <pre><code>$ pwd\n/users/xx999x/Documents\n$ cd ..\n$ pwd\n/users/xx999x\n</code></pre> <p>The symbol <code>~</code> (tilde) is used to refer to the home directory of a user. You can refer to this in your paths:</p> <pre><code>$ cd Images\n$ pwd\n/users/xx999x/Images\n$ cd ~/Documents\n$ pwd\n/users/xx999x/Documents\n</code></pre>"},{"location":"tutorials/unix-101/#lesson-2-managing-files","title":"Lesson 2: Managing Files","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 oldFile.txt\n      \u2514 ImportantFiles\n        \u2514 importantFile.txt\n    \u2514 Documents\n      \u2514 myFile.txt\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home <code>/users/xx999x</code>.</p>"},{"location":"tutorials/unix-101/#creating-directories","title":"Creating Directories","text":"<p>You can also expand the file structure by creating your own directories, as long as you have permission to do so, which within your home you should always have!</p> <p>For this use the <code>mkdir</code> command, where mkdir stands for make directory.</p> <pre><code>$ ls\nDocuments Images\n$ mkdir Data\n$ ls\nData Documents Images\n</code></pre>"},{"location":"tutorials/unix-101/#copying-files","title":"Copying Files","text":"<p>To copy a file you can use the <code>cp</code> command, where cp stands for copy. The first argument after the command is the source file, which you want to copy. The second argument is the destination, that can be a directory or a path to a filename, which the copied file will get renamed to:</p> <pre><code>$ cp Documents/myFile.txt Data/\n$ ls Data\nmyFile.txt\n$ cp Documents/myFile.txt Data/copyOfMyFile.txt\n$ ls Data\ncopyOfMyFile.txt myFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#moving-renaming-files","title":"Moving / Renaming Files","text":"<p>To move files around use the <code>mv</code> command, where mv stands for move. The first argument after the command is the source file, which you want to move. The second argument is the destination, that can be a directory or a path to a name, which the moved file will get renamed to:</p> <pre><code>$ mv Documents/myFile.txt Data/\n$ ls Data\nmyFile.txt\n</code></pre> <p>You can also use the <code>mv</code> utility to rename a file, by moving it into the same place but choosing a different destination name:</p> <pre><code>$ ls Data\nmyFile.txt\n$ mv Data/myFile.txt Data/myFileMoved.txt\n$ ls Data\nmyFileMoved.txt\n</code></pre>"},{"location":"tutorials/unix-101/#removing-items","title":"Removing Items","text":"<p>To remove files and directories you can use the <code>rm</code> command, where rm stands for remove. Be careful though, as this action can not be undone!</p> <p>For a single file you can just supply it as an argument:</p> <pre><code>$ ls Data\nImportantFiles oldFile.txt \n$ rm Data/oldFile.txt\n$ ls Data\nImportantFiles\n</code></pre> <p>If you want to remove a whole directory you have to use the <code>-r</code> parameter, to remove recursively.</p> <pre><code>$ ls Data\nImportantFiles\n$ rm -r /Data/ImportantFiles\n$ ls Data\n</code></pre>"},{"location":"tutorials/unix-101/#lesson-3-manipulating-text-files","title":"Lesson 3: Manipulating Text Files","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 petData.csv\n    \u2514 Documents\n    \u2514 Images\n\u2514 mnt\n  \u2514 data\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home <code>/users/xx999x</code>.</p>"},{"location":"tutorials/unix-101/#creating-empty-files","title":"Creating empty files","text":"<p>To create empty files you can use the <code>touch</code> utility. <code>touch</code> is a software that changes a file\u2019s timestamp, but if the file supplied does not exist already, it will create it.</p> <pre><code>$ ls\nData Documents Images\n$ touch myFile.txt\n$ ls\nData Documents Images myFile.txt\n</code></pre>"},{"location":"tutorials/unix-101/#viewing-files","title":"Viewing files","text":"<p>You don\u2019t have to open a file in an editor to see its contents, you can use the <code>cat</code> utility, where cat stands for concatenate.</p> <pre><code>$ cat Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n</code></pre> <p>If you only want to see the beginning of a file, you can use the <code>head</code> command. With <code>-</code> and any number you can choose how many lines from the top you want to display:</p> <pre><code>$ head -2 Data/petData.csv\nname,animal,age\nFluffy,dog,3\n$ head -4 Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\n</code></pre> <p>If you only want to see the end of a file, you can use the <code>tail</code> command. With <code>-</code> and any number you can choose how many lines from the bottom you want to display:</p> <pre><code>$ tail -2 Data/petData.csv\nPrincess,horse,12\nSnickers,dog,6\n$ tail -4 Data/petData.csv\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n</code></pre> <p>To remember these three commands, just think of the silhouette of a cat. If you want to see the whole cat, use <code>cat</code>. If you only want to see the top, use <code>head</code>. And if you only want to see the bottom use <code>tail</code>:</p>"},{"location":"tutorials/unix-101/#search-in-file","title":"Search in file","text":"<p>To search for a term or phrase within a file use the <code>grep</code> command. It returns you each line within a file, that matches the term you searched for and depending on your console settings also marks the word. Use the <code>-i</code> parameter to search case-insensitive:</p> <pre><code>$ cat Data/petData.csv\nname,animal,age\nFluffy,dog,3\nMilky,cat,8\nRudolph,duck,2\nHank,cat,6\nPrincess,horse,12\nSnickers,dog,6\n$ grep -i \"dog\" Data/petData.csv\nFluffy,dog,3\nSnickers,dog,6\n</code></pre> <p>You can chain additional <code>grep</code> commands, to further search your results, just separate them with <code>|</code>.</p> <pre><code>$ grep -i \"dog\" Data/petData.csv\nFluffy,dog,3\nSnickers,dog,6\n$ grep -i \"dog\" Data/petData.csv | grep \"6\"\nSnickers,dog,6\n</code></pre>"},{"location":"tutorials/unix-101/#writing-output-to-file","title":"Writing output to file","text":"<p>You can save output of a command in UNIX to a file, instead of printing it to the console. This is helpful, when you run a command, that creates a lot of output you\u2019ll need to analyse later.</p> <p>The following example with <code>&gt;</code> writes the output (called <code>STDOUT</code>) of a command to a file and overwrites all existing content in that file. If the file does not exist already, it will be created.</p> <pre><code>$ ls\nData Documents Images\n$ ls &gt; myWorkDirectory.txt\n$ ls\nData Documents Images myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\n$ ls &gt; myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images myWorkDirectory.txt\n</code></pre> <p>The example below with <code>&gt;&gt;</code> writes the output (called <code>STDOUT</code>) of a command to a file and appends it to the existing content in that file. If the file does not exist already, it will be created.</p> <pre><code>$ ls\nData Documents Images\n$ ls &gt;&gt; myWorkDirectory.txt\n$ ls\nData Documents Images myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\n$ ls &gt;&gt; myWorkDirectory.txt\n$ cat myWorkDirectory.txt\nData Documents Images\nData Documents Images myWorkDirectory.txt\n</code></pre>"},{"location":"tutorials/unix-101/#edit-file","title":"Edit file","text":"<p>To edit files, you can use a file editor. The most beginner friendly file editor is <code>nano</code>. If you run <code>nano</code> you will get put into an interactive program and lose access to the console:</p> <pre><code>$ nano Data/petData.csv\n</code></pre> <p>In your interactive editor, you can move around the file using the arrow keys on your keyboard and edit the file.</p> <p>After you are done making your changes, you can press <code>ctrl + X</code>, this will prompt the application to exit. Press <code>Y</code> and then <code>Enter</code> to save your changes or <code>N</code> to discard them.</p>"},{"location":"tutorials/unix-101/#lesson-4-filesystem-permissions","title":"Lesson 4: Filesystem Permissions","text":"<p>As an example for this lesson, let\u2019s take this structure:</p> <pre><code>/\n\u2514 users\n  \u2514 999999x\n  \u2514 xx999x\n    \u2514 Data\n      \u2514 myFile.txt\n\u2514 mnt\n  \u2514 data\n    \u2514 project9999\n      \u2514 ourResearchFile.txt\n\u2514 tmp\n</code></pre> <p>You are user <code>xx999x</code> and therefore your journey starts in your home <code>/users/xx999x</code>.</p> <p>Every file and directory in UNIX has permissions. They are separated in three levels:</p> <ul> <li>The User (<code>u</code>) is usually the creator of the file or directory, only root can change this.</li> <li>The Group (<code>g</code>) is used to potentially share data with other groups of users.</li> <li>Other (<code>o</code>) is any other user on the system, this should be the layer that is most restricted.</li> </ul> <p>Permissions are denoted in a string of 10 characters. The first 1 is to identify the type of file. This can be either of those:</p> <ul> <li><code>d</code> for directories</li> <li><code>l</code> for symbolic links</li> <li><code>-</code> for an ordinary file</li> </ul> <p>The other 9 to determine the permission for every level described above. Each level has 3 characters dedicated:</p> <ul> <li><code>r</code> for read permission</li> <li><code>w</code> for write permission</li> <li><code>x</code> for execute permission (needed to run scripts or enter directories)</li> </ul> <p>If any of the characters are a <code>-</code> instead, that means that this permission is not granted on that level.</p> <p>Lets look at some examples of this:</p> Permission Explanation <code>drwx------</code> This is a directory with full permissions only on the user level. This is the permission your home directory would have. <code>lrwxrwxrwx</code> This is a symbolic link. Due to the nature of them always inheriting the permission of the resource they link to, they show as having full permission on all levels. <code>-rw-rw-r--</code> This is a file with read + write permissions on the user and group level, and only read permissions on the other level. This is the default permission if you create a new file. <p>If you use the <code>-l</code> parameter for <code>ls</code>, you will get a long list output for a file or the components of a directory.</p> <pre><code>$ ls -l Data/myFile.txt\n-rw-rw-r-- 1 &lt;user&gt; &lt;group&gt; &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre> <p>Here you can see who is the user / group level owner and the 10 character string in action!</p>"},{"location":"tutorials/unix-101/#changing-permissions","title":"Changing permissions","text":"<p>To change the permissions you can use the <code>chmod</code> command, where chmod stands for change mode. Define the level you want to modify (if nothing is provided, all levels get adjusted), the type of modification you are doing (<code>+</code>/<code>-</code>) and the permissions.</p> <pre><code>$ ls -l Data/myFile.txt\n-rw-rw-r-- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod o-r Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rw-rw---- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod ug+rwx Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwxrwx--- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod -x Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rw-rw---- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre> <p>As mentioned earlier, you can\u2019t change the owner of a file without <code>root</code> permissions, however you can change group. For this use the <code>chown</code> command, where chown stands for change owner. When changing the group permission you have to put a <code>:</code> before the owner group name:</p> <pre><code>$ ls -l /mnt/data/project9999/ourResearchFile.txt\n-rw-rw-r-- 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/ourResearchFile.txt\n$ chown :project9999 /mnt/data/project9999/ourResearchFile.txt\n$ ls -l /mnt/data/project9999/ourResearchFile.txt\n-rw-rw-r-- 1 xx999x project9999 &lt;size&gt; &lt;date-time&gt; Data/ourResearchFile.txt\n</code></pre> <p>This can be used to share files with your fellow project members.</p>"},{"location":"tutorials/unix-101/#special-permissions","title":"Special permissions","text":"<p>Special permissions are used to manipulate permission functionality to a file or directory and can be set on all three levels, each with different effects:</p>"},{"location":"tutorials/unix-101/#user-suid","title":"User (suid)","text":"<p>When set, the file is always run as the user owning the file, no matter who runs it. A well known example would be <code>/usr/bin/passwd</code>, which always runs as <code>root</code>.</p> <p>This configuration is shown as an <code>s</code> instead of an <code>x</code> on the user level:</p> <pre><code>$ ls -l /usr/bin/passwd \n-rwsr-xr-x. 1 root root &lt;size&gt; &lt;date-time&gt; /usr/bin/passwd\n</code></pre>"},{"location":"tutorials/unix-101/#group-sgid","title":"Group (sgid)","text":"<p>If set on a file, the file can be run as the group. The more common way to use it is on a directory, where if sgid is set, group ownership of files created is set the the directory group owner. You will find this configuration is standard in shared directories.</p> <p>This configuration is shown as an <code>s</code> instead of an <code>x</code> on the group level:</p> <pre><code>$ ls -l /mnt/data/\ndrwxrws--- 4 root project9999 &lt;size&gt; &lt;date-time&gt; project9999\n</code></pre>"},{"location":"tutorials/unix-101/#other-sticky-bit","title":"Other (sticky bit)","text":"<p>This permission does not affect files. On directories however, it restricts file deletion and renaming. Only the owner user of a file can remove them within such a directory. This is most commonly used in <code>/tmp</code>, to allow everybody to write in it, but prevent others from deleting your work.</p> <p>This configuration is shown as a <code>t</code> instead of an <code>x</code> on the other level:</p> <pre><code>$ ls -la /\ndrwxrwxrwt. 1980 root root &lt;size&gt; &lt;date-time&gt; tmp\n</code></pre> <p>To set these special permissions, you can also use the <code>chmod</code> command. On either the user or group level set <code>s</code> for suid or sgid. For the sticky bit you need to use <code>t</code>:</p> <pre><code>$ ls -l Data/myFile.txt\n-rwxrwxrwx 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod ug+s Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwsrwsrwx 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n$ chmod +t Data/myFile.txt\n$ ls -l Data/myFile.txt\n-rwsrwsrwt 1 xx999x clusterusers &lt;size&gt; &lt;date-time&gt; Data/myFile.txt\n</code></pre>"}]}